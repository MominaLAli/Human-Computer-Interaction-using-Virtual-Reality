# Natural Human-Computer Interface Based on Gesture Recognition with YOLO to enhance user experience
This research work is motivated by the gaining popularity of virtal reality in various domains of life specially after COVID-19. In this work we have created an opencv based application to control mouse and keyboard which can be used to play games on compuer, control media virtually and also for distance education environments. We used the capabilities of YOLO to perform hand detection in real time.
## Running the code:
1- Clone the repo.

2- Upload the notebook to your colab account.

3- You can begin with any of the notebooks. If you want to generate annotations again run the annotations notebook and give paths in the drive to save annotations in json format.

4- The drive paths are given in the notebook so my data can be used just by running the gdown cells.

5- For model training use the nano notebook and the notebook is connected ot drive so all the data is taken from the drive and after every 10 epochs the weights are being saved in drive.

6- Import the best weights and set the path in virtual mouse and keyboard codes.

7- You can use the virtual mouse and keyboard freely.

## Useful Resources:
1- https://github.com/jin-s13/COCO-WholeBody/
2- https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose
3- https://github.com/Deci-AI/super-gradients/tree/master
4- Paper: "RTMDet: An Empirical Study of Designing Real-Time Object Detectors"
5- Paper: "RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose"
6- Best resources to learn YOLO-NAS Pose->  https://www.linkedin.com/pulse/8-community-created-content-get-started-yolo-nas-pose-deciai-omguc/

